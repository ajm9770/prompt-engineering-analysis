{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Analysis with Anthropic's Claude\n",
    "\n",
    "This notebook analyzes how different prompt engineering techniques affect Claude's output distributions, response characteristics, and behavior.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Setting up the Anthropic API\n",
    "2. Comparing different prompt engineering techniques\n",
    "3. Analyzing temperature effects on output distribution\n",
    "4. Visualizing output diversity and characteristics\n",
    "5. Statistical analysis of prompt effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import our analyzer\n",
    "from prompt_engineering_analyzer import PromptEngineeringAnalyzer\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Key\n",
    "\n",
    "Make sure you have your Anthropic API key set as an environment variable:\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY='your-api-key-here'\n",
    "```\n",
    "\n",
    "Or set it directly in this notebook (not recommended for production):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use environment variable (recommended)\n",
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Option 2: Set directly (uncomment and use with caution)\n",
    "# api_key = \"your-api-key-here\"\n",
    "\n",
    "if not api_key:\n",
    "    print(\"⚠️ WARNING: ANTHROPIC_API_KEY not found!\")\n",
    "    print(\"Please set your API key before proceeding.\")\n",
    "else:\n",
    "    print(\"✓ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer with your preferred model\n",
    "analyzer = PromptEngineeringAnalyzer(\n",
    "    api_key=api_key,\n",
    "    model=\"claude-3-5-sonnet-20241022\"  # or \"claude-3-opus-20240229\", etc.\n",
    ")\n",
    "\n",
    "print(f\"Analyzer initialized with model: {analyzer.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Prompt Testing\n",
    "\n",
    "Let's start with a simple example to see how Claude responds to a basic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple prompt\n",
    "test_prompt = \"Explain what a neural network is in one sentence.\"\n",
    "\n",
    "response = analyzer.get_response(test_prompt, temperature=0.7)\n",
    "\n",
    "print(\"Prompt:\", test_prompt)\n",
    "print(\"\\nResponse:\", response['response_text'])\n",
    "print(\"\\nMetadata:\")\n",
    "print(f\"  - Input tokens: {response['input_tokens']}\")\n",
    "print(f\"  - Output tokens: {response['output_tokens']}\")\n",
    "print(f\"  - Response length: {response['response_length']} characters\")\n",
    "print(f\"  - Stop reason: {response['stop_reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Prompt Engineering Techniques\n",
    "\n",
    "Now let's compare different prompt engineering approaches on the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your question\n",
    "base_question = \"What are the three most important factors in training a deep learning model?\"\n",
    "\n",
    "# Define different prompt engineering variants\n",
    "prompt_variants = {\n",
    "    \"baseline\": {\n",
    "        \"prompt\": base_question,\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"with_system_prompt\": {\n",
    "        \"prompt\": base_question,\n",
    "        \"system\": \"You are an expert deep learning researcher with 10 years of experience.\"\n",
    "    },\n",
    "    \n",
    "    \"chain_of_thought\": {\n",
    "        \"prompt\": f\"{base_question}\\n\\nLet's think through this step by step:\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"structured\": {\n",
    "        \"prompt\": f\"{base_question}\\n\\nProvide your answer in this format:\\n1. [Factor]: [Explanation]\\n2. [Factor]: [Explanation]\\n3. [Factor]: [Explanation]\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"few_shot\": {\n",
    "        \"prompt\": f\"\"\"Here's an example of explaining key factors:\n",
    "\n",
    "Q: What are the three most important factors in building a web application?\n",
    "A: The three most important factors are:\n",
    "1. Security - Protecting user data and preventing vulnerabilities\n",
    "2. Performance - Ensuring fast load times and responsive interactions\n",
    "3. User Experience - Creating an intuitive and accessible interface\n",
    "\n",
    "Q: {base_question}\n",
    "A:\"\"\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"role_playing\": {\n",
    "        \"prompt\": base_question,\n",
    "        \"system\": \"You are Yann LeCun, discussing deep learning with a colleague. Be concise but insightful.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Prompt variants defined:\")\n",
    "for name in prompt_variants.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the comparison (this will take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all variants\n",
    "print(\"Running comparison... This may take a few minutes.\\n\")\n",
    "\n",
    "df_variants = analyzer.compare_prompts(\n",
    "    base_question=base_question,\n",
    "    prompt_variants=prompt_variants,\n",
    "    temperature=0.7,\n",
    "    num_samples=5  # 5 samples per variant\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Comparison complete!\")\n",
    "print(f\"\\nTotal responses collected: {len(df_variants)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View sample responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one sample from each variant\n",
    "for variant in df_variants['variant'].unique():\n",
    "    sample = df_variants[df_variants['variant'] == variant].iloc[0]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Variant: {variant}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Response:\\n{sample['response_text']}\")\n",
    "    print(f\"\\nTokens: {sample['output_tokens']} | Length: {sample['response_length']} chars\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze output diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_metrics = analyzer.analyze_output_diversity(df_variants)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "diversity_df = pd.DataFrame(diversity_metrics).T\n",
    "diversity_df = diversity_df.round(3)\n",
    "\n",
    "print(\"Output Diversity Metrics:\\n\")\n",
    "print(diversity_df)\n",
    "\n",
    "# Highlight key insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "most_diverse = diversity_df['uniqueness_ratio'].idxmax()\n",
    "least_diverse = diversity_df['uniqueness_ratio'].idxmin()\n",
    "\n",
    "print(f\"\\nMost diverse responses: {most_diverse} (ratio: {diversity_df.loc[most_diverse, 'uniqueness_ratio']:.3f})\")\n",
    "print(f\"Least diverse responses: {least_diverse} (ratio: {diversity_df.loc[least_diverse, 'uniqueness_ratio']:.3f})\")\n",
    "\n",
    "longest = diversity_df['avg_response_length'].idxmax()\n",
    "shortest = diversity_df['avg_response_length'].idxmin()\n",
    "\n",
    "print(f\"\\nLongest responses: {longest} ({diversity_df.loc[longest, 'avg_response_length']:.0f} chars avg)\")\n",
    "print(f\"Shortest responses: {shortest} ({diversity_df.loc[shortest, 'avg_response_length']:.0f} chars avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.visualize_prompt_comparison(df_variants, save_path=\"prompt_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Analysis\n",
    "\n",
    "Now let's see how temperature affects the output distribution for a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt for temperature analysis\n",
    "temp_test_prompt = \"Name three creative uses for artificial intelligence in healthcare.\"\n",
    "\n",
    "print(\"Running temperature analysis...\\n\")\n",
    "print(f\"Prompt: {temp_test_prompt}\")\n",
    "print(f\"Temperatures to test: [0.0, 0.3, 0.7, 1.0, 1.5]\\n\")\n",
    "\n",
    "df_temperature = analyzer.analyze_temperature_effects(\n",
    "    prompt=temp_test_prompt,\n",
    "    system=\"You are a creative AI assistant.\",\n",
    "    temperatures=[0.0, 0.3, 0.7, 1.0, 1.5],\n",
    "    num_samples=10\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Temperature analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View responses at different temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one example from each temperature\n",
    "for temp in sorted(df_temperature['temperature'].unique()):\n",
    "    sample = df_temperature[df_temperature['temperature'] == temp].iloc[0]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(\"=\"*80)\n",
    "    print(sample['response_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics by temperature\n",
    "temp_stats = df_temperature.groupby('temperature').agg({\n",
    "    'response_length': ['mean', 'std', 'min', 'max'],\n",
    "    'word_count': ['mean', 'std'],\n",
    "    'output_tokens': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistics by Temperature:\\n\")\n",
    "print(temp_stats)\n",
    "\n",
    "# Calculate uniqueness ratio by temperature\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Response Uniqueness by Temperature:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in sorted(df_temperature['temperature'].unique()):\n",
    "    responses = df_temperature[df_temperature['temperature'] == temp]['response_text'].tolist()\n",
    "    unique_ratio = len(set(responses)) / len(responses)\n",
    "    print(f\"Temperature {temp}: {unique_ratio:.2%} unique responses ({len(set(responses))}/{len(responses)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize temperature effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.visualize_temperature_effects(df_temperature, save_path=\"temperature_effects.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Analysis: Response Patterns\n",
    "\n",
    "Let's do some custom analysis on common patterns in the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response_patterns(df, text_column='response_text'):\n",
    "    \"\"\"\n",
    "    Analyze common patterns in responses.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'starts_with_number': 0,\n",
    "        'contains_list': 0,\n",
    "        'contains_bullet': 0,\n",
    "        'starts_with_capital': 0,\n",
    "        'contains_colon': 0,\n",
    "    }\n",
    "    \n",
    "    for text in df[text_column]:\n",
    "        if text[0].isdigit():\n",
    "            patterns['starts_with_number'] += 1\n",
    "        if any(text.startswith(f\"{i}.\") or f\"\\n{i}.\" in text for i in range(1, 10)):\n",
    "            patterns['contains_list'] += 1\n",
    "        if '•' in text or '- ' in text or '* ' in text:\n",
    "            patterns['contains_bullet'] += 1\n",
    "        if text[0].isupper():\n",
    "            patterns['starts_with_capital'] += 1\n",
    "        if ':' in text:\n",
    "            patterns['contains_colon'] += 1\n",
    "    \n",
    "    # Convert to percentages\n",
    "    total = len(df)\n",
    "    for key in patterns:\n",
    "        patterns[key] = (patterns[key] / total) * 100\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Analyze patterns by variant\n",
    "print(\"Response Patterns by Variant:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for variant in df_variants['variant'].unique():\n",
    "    variant_df = df_variants[df_variants['variant'] == variant]\n",
    "    patterns = analyze_response_patterns(variant_df)\n",
    "    \n",
    "    print(f\"\\n{variant}:\")\n",
    "    for pattern, percentage in patterns.items():\n",
    "        print(f\"  {pattern}: {percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis\n",
    "\n",
    "Analyze which words appear most frequently in different prompt variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_top_words(df, variant, n=20, min_length=4):\n",
    "    \"\"\"\n",
    "    Get top N words for a specific variant.\n",
    "    \"\"\"\n",
    "    # Get all responses for this variant\n",
    "    texts = df[df['variant'] == variant]['response_text'].tolist()\n",
    "    \n",
    "    # Combine all text\n",
    "    combined_text = ' '.join(texts).lower()\n",
    "    \n",
    "    # Extract words (alphanumeric only)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', combined_text)\n",
    "    \n",
    "    # Filter by length and remove common stop words\n",
    "    stop_words = {'the', 'this', 'that', 'with', 'from', 'have', 'they', 'will', 'your', \n",
    "                  'more', 'about', 'which', 'their', 'there', 'than', 'them', 'these',\n",
    "                  'been', 'were', 'when', 'where', 'also', 'can', 'are', 'and', 'for'}\n",
    "    \n",
    "    words = [w for w in words if len(w) >= min_length and w not in stop_words]\n",
    "    \n",
    "    # Count and return top N\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Analyze top words for each variant\n",
    "print(\"Top 15 Words by Variant:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for variant in df_variants['variant'].unique():\n",
    "    print(f\"\\n{variant}:\")\n",
    "    top_words = get_top_words(df_variants, variant, n=15)\n",
    "    \n",
    "    for word, count in top_words:\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Word Frequency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison of top words across variants\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, variant in enumerate(df_variants['variant'].unique()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "        \n",
    "    top_words = get_top_words(df_variants, variant, n=10)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    axes[idx].barh(words, counts, color=sns.color_palette(\"husl\", 10))\n",
    "    axes[idx].set_xlabel('Frequency')\n",
    "    axes[idx].set_title(f'Top Words: {variant}')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(len(df_variants['variant'].unique()), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word_frequency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "analyzer.save_results(df_variants, \"prompt_variants_results\")\n",
    "analyzer.save_results(df_temperature, \"temperature_analysis_results\")\n",
    "\n",
    "print(\"Results saved!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - prompt_variants_results.csv\")\n",
    "print(\"  - prompt_variants_results.json\")\n",
    "print(\"  - temperature_analysis_results.csv\")\n",
    "print(\"  - temperature_analysis_results.json\")\n",
    "print(\"  - prompt_comparison.png\")\n",
    "print(\"  - temperature_effects.png\")\n",
    "print(\"  - word_frequency_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Your Custom Experiments\n",
    "\n",
    "Use this section to run your own experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own question and variants here\n",
    "\n",
    "my_question = \"Your question here\"\n",
    "\n",
    "my_variants = {\n",
    "    \"variant_1\": {\n",
    "        \"prompt\": my_question,\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    # Add more variants...\n",
    "}\n",
    "\n",
    "# Run your experiment\n",
    "# my_results = analyzer.compare_prompts(my_question, my_variants, temperature=0.7, num_samples=5)\n",
    "# analyzer.visualize_prompt_comparison(my_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Prompt Engineering Techniques**: How different prompting strategies (zero-shot, few-shot, chain-of-thought, etc.) affect outputs\n",
    "2. **Temperature Effects**: How temperature influences output diversity and creativity\n",
    "3. **Output Analysis**: Measuring response length, diversity, and patterns\n",
    "4. **Word Frequency**: Understanding vocabulary usage across different prompts\n",
    "\n",
    "Key findings you might observe:\n",
    "- Lower temperatures (0.0-0.3) produce more consistent, deterministic outputs\n",
    "- Higher temperatures (1.0-1.5) increase diversity but may reduce coherence\n",
    "- Structured prompts tend to produce more consistent formatting\n",
    "- System prompts can significantly influence tone and style\n",
    "- Few-shot examples guide the model toward specific response patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
